{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/david3684/2024_arithmetic/src\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "module_path = os.path.abspath(os.path.join('/data2/david3684/2024_arithmetic'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from src.eval import eval_single_dataset_with_prediction, eval_single_dataset\n",
    "from src.main import save_scale_factors\n",
    "from src.args import parse_arguments\n",
    "from src.datasets.common import get_dataloader, maybe_dictionarize\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.modeling import ImageEncoder, ImageClassifier\n",
    "from src.task_vectors import TaskVector\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = 'ViT-L-14'\n",
    "        self.tasks = ['DTD', 'SUN397']\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.task_scale_factors = None\n",
    "        self.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "        self.data_location = '/data2/david3684/data'\n",
    "        self.eval_datasets = None\n",
    "        self.train_dataset = None\n",
    "        self.exp_name = None\n",
    "        self.results_db = None\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.001\n",
    "        self.wd = 0.1\n",
    "        self.ls = 0.0\n",
    "        self.warmup_length = 500\n",
    "        self.epochs = 10\n",
    "        self.load = None\n",
    "        self.cache_dir = None\n",
    "        self.openclip_cachedir = '/data2/david3684/.cache/open_clip'\n",
    "        self.initial_rank_ratio = 1.0\n",
    "        self.low_rank_mode = 'SoRA'\n",
    "        self.pretrained_model = 'openai'\n",
    "        self.scale_shared_weight = False\n",
    "        self.num_test_samples = 2048\n",
    "        self.no_shared_weight = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned weight\n",
    "\n",
    "model_1 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/DTD/finetuned.pt').to(args.device)\n",
    "model_2 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/SUN397/finetuned.pt').to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(old_key):\n",
    "    if old_key.startswith('shared.attn.layer') or old_key.startswith('clip_vit'):\n",
    "        parts = old_key.split('.')\n",
    "        layer_idx = parts[3]\n",
    "        # print(layer_idx)\n",
    "        sub_key = parts[4]\n",
    "        if sub_key in ['q', 'k', 'v']:\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.{sub_key}_weight'\n",
    "        elif sub_key == 'out_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.out_proj.weight'\n",
    "        elif sub_key == 'c_fc' or sub_key == 'c_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.mlp.{sub_key}.weight'\n",
    "    return old_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scale_factors(scale_dict):\n",
    "    qkv_scale_store_task1 = {}\n",
    "    qkv_scale_store_task2 = {}\n",
    "    scale_factors_1 = {}\n",
    "    scale_factors_2 = {}\n",
    "    for scale_dict_key, value in scale_dict.items():\n",
    "        transformed_scale_dict_key = transform_key(scale_dict_key)\n",
    "        if 'clip_vit_1' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task1:\n",
    "                qkv_scale_store_task1[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_1[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "        elif 'clip_vit_2' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task2:\n",
    "                qkv_scale_store_task2[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_2[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        # print(layer_idx, qkv)\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # print('hi')\n",
    "            scale_factors_1[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight.scale'] = concat_scale\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            scale_factors_2[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight' +\n",
    "                            '.scale'] = concat_scale\n",
    "\n",
    "    return scale_factors_1, scale_factors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shared_weight(shared_weight_state_dict, open_clip_state_dict_template):\n",
    "    qkv_store = {}\n",
    "    for old_key, value in shared_weight_state_dict.items():\n",
    "        if 'diff' in old_key or 'scale_dict' in old_key:\n",
    "            continue\n",
    "\n",
    "        new_key = transform_key(old_key)\n",
    "        layer_idx = new_key.split('.')[4]\n",
    "\n",
    "        if layer_idx not in qkv_store:\n",
    "            qkv_store[layer_idx] = {'q': None, 'k': None, 'v': None}\n",
    "\n",
    "        weight_type = new_key.split('.')[-1]\n",
    "        # in_proj.weight (q, k, v)\n",
    "        if weight_type in ['q_weight', 'k_weight', 'v_weight']:\n",
    "            if args.scale_shared_weight:\n",
    "                scale_key = f'shared.attn.layer.{layer_idx}.{weight_type[0]}'\n",
    "                if scale_key in shared_weight_state_dict['scale_dict']:\n",
    "                    weight_scale_factor = shared_weight_state_dict['scale_dict'][scale_key]\n",
    "                    scaled_value = value / weight_scale_factor\n",
    "                    qkv_store[layer_idx][weight_type[0]] = scaled_value\n",
    "                else:\n",
    "                    print(f\"Scale key {scale_key} not found in scale_dict.\")\n",
    "            else:\n",
    "                qkv_store[layer_idx][weight_type[0]] = value\n",
    "        else:  # out_proj.weight, c_fc.weight, c_proj.weight\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            weight_scale_factor = shared_weight_state_dict['scale_dict'][old_key]\n",
    "            open_clip_state_dict_template[new_key] = value / \\\n",
    "                weight_scale_factor\n",
    "\n",
    "    for layer_idx, qkv in qkv_store.items():\n",
    "        if all(v.bool().all().item() for v in qkv.values()):\n",
    "            in_proj_weight = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # concat qkv into 3072*1024 tensor\n",
    "            new_key = f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight'\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = in_proj_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Missing q, k, or v for layer {layer_idx}. q: {qkv['q']}, k: {qkv['k']}, v: {qkv['v']}\")\n",
    "\n",
    "    return open_clip_state_dict_template\n",
    "\n",
    "#나머지 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisualTransformer(\n",
       "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-23): 24 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_attn): Identity()\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (ln): Identity()\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format shaed weight into openclip state dict\n",
    "shared_weight_state_dict = torch.load('/data2/david3684/2024_arithmetic/shared_weight/20241007_vanilla/rankmin_config_20241009_uni_vanilla_2.bin')\n",
    "\n",
    "scale_factors_1, scale_factors_2 = save_scale_factors(\n",
    "    shared_weight_state_dict['scale_dict'])\n",
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "\n",
    "zero_shot_encoder = ImageEncoder(args, keep_lang=False)\n",
    "\n",
    "formatted_shared_weight = format_shared_weight(shared_weight_state_dict, zero_shot_encoder.state_dict())\n",
    "\n",
    "zero_shot_encoder.load_state_dict(formatted_shared_weight)\n",
    "zero_shot_encoder.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Classification head for ViT-L-14 on DTD exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:32<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on DTD. Accuracy: 2.13%\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 21751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [04:17<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SUN397. Accuracy: 0.13%\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Classification head for ViT-L-14 on DTD exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:30<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on DTD. Accuracy: 2.13%\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 21751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 20/170 [00:40<03:34,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "# loop for task vector rank\n",
    "\n",
    "for initial_rank_ratio in [1.0, 0.5, 1.0]:\n",
    "    low_rank_task_vectors = {}\n",
    "    \n",
    "    # Build low rank task vectors\n",
    "    for task in args.tasks:\n",
    "        model = model_1 if task == 'DTD' else model_2\n",
    "        # eval_single_dataset(model, task, args)\n",
    "        finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "        low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task).to(args.device)\n",
    "    \n",
    "    low_rank_task_vector_sum = sum(low_rank_task_vectors.values())\n",
    "    \n",
    "    \n",
    "    for task in args.tasks:\n",
    "        # Evaluate sinlge task model\n",
    "        low_rank_single_task_encoder = low_rank_task_vectors[task].apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "        # eval_single_dataset(low_rank_single_task_encoder, task, args)\n",
    "        \n",
    "        # Evaluate multi task model\n",
    "        low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "        eval_single_dataset(low_rank_multi_task_encoder, task, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 2048\n"
     ]
    }
   ],
   "source": [
    "# _, _, val_preprocess = open_clip.create_model_and_transforms(\n",
    "#             args.model, pretrained='openai', cache_dir=args.openclip_cachedir)\n",
    "# dataset_1 = get_dataset(\n",
    "#         args.tasks[0],\n",
    "#         val_preprocess,\n",
    "#         location=args.data_location,\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=16,\n",
    "#         num_test_samples=None,\n",
    "#     )\n",
    "# dataloader_1 = get_dataloader(\n",
    "#     dataset_1, is_train=False, args=args, image_encoder=None)\n",
    "\n",
    "# dataset_2 = get_dataset(\n",
    "#         args.tasks[1],\n",
    "#         val_preprocess,\n",
    "#         location=args.data_location,\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=16,\n",
    "#         num_test_samples=args.num_test_samples,\n",
    "#     )\n",
    "# dataloader_2 = get_dataloader(\n",
    "#     dataset_2, is_train=False, args=args, image_encoder=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
