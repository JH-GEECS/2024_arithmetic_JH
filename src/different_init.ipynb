{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/david3684/2024_arithmetic/src\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "module_path = os.path.abspath(os.path.join('/data2/david3684/2024_arithmetic'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from src.eval import eval_single_dataset_with_prediction, eval_single_dataset\n",
    "from src.main import save_scale_factors\n",
    "from src.args import parse_arguments\n",
    "from src.datasets.common import get_dataloader, maybe_dictionarize\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.modeling import ImageEncoder, ImageClassifier\n",
    "from src.task_vectors import TaskVector\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = 'ViT-L-14'\n",
    "        self.tasks = ['DTD', 'SUN397']\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.task_scale_factors = None\n",
    "        self.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "        self.data_location = '/data2/david3684/data'\n",
    "        self.eval_datasets = None\n",
    "        self.train_dataset = None\n",
    "        self.exp_name = None\n",
    "        self.results_db = None\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.001\n",
    "        self.wd = 0.1\n",
    "        self.ls = 0.0\n",
    "        self.warmup_length = 500\n",
    "        self.epochs = 10\n",
    "        self.load = None\n",
    "        self.cache_dir = None\n",
    "        self.openclip_cachedir = '/data2/david3684/.cache/open_clip'\n",
    "        self.initial_rank_ratio = 1.0\n",
    "        self.low_rank_mode = 'SoRA'\n",
    "        self.pretrained_model = 'openai'\n",
    "        self.scale_shared_weight = True\n",
    "        self.no_shared_weight = True\n",
    "        self.num_test_samples = 2048\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/DTDVal/finetuned_laion2b_s32b_b82k.pt').to(args.device)\n",
    "model_2 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/SUN397/finetuned.pt').to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(state_dict1, state_dict2):\n",
    "    \"\"\"Average the weights of two state dicts.\"\"\"\n",
    "    averaged_state_dict = {}\n",
    "    for key in state_dict1:\n",
    "        averaged_state_dict[key] = (state_dict1[key] + state_dict2[key]) / 2\n",
    "    return averaged_state_dict\n",
    "\n",
    "def create_model_with_averaged_weights(args, state_dict1, state_dict2):\n",
    "    \"\"\"Create a model with averaged weights.\"\"\"\n",
    "    averaged_state_dict = average_weights(state_dict1, state_dict2)\n",
    "    model = ImageEncoder(args, keep_lang=False)\n",
    "    model.load_state_dict(averaged_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "averaged_model = create_model_with_averaged_weights(args, model_1.state_dict(), model_2.state_dict()).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(old_key):\n",
    "    if old_key.startswith('shared.attn.layer') or old_key.startswith('clip_vit'):\n",
    "        parts = old_key.split('.')\n",
    "        layer_idx = parts[3]\n",
    "        # print(layer_idx)\n",
    "        sub_key = parts[4]\n",
    "        if sub_key in ['q', 'k', 'v']:\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.{sub_key}_weight'\n",
    "        elif sub_key == 'out_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.out_proj.weight'\n",
    "        elif sub_key == 'c_fc' or sub_key == 'c_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.mlp.{sub_key}.weight'\n",
    "    return old_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scale_factors(scale_dict):\n",
    "    qkv_scale_store_task1 = {}\n",
    "    qkv_scale_store_task2 = {}\n",
    "    scale_factors_1 = {}\n",
    "    scale_factors_2 = {}\n",
    "    for scale_dict_key, value in scale_dict.items():\n",
    "        transformed_scale_dict_key = transform_key(scale_dict_key)\n",
    "        if 'clip_vit_1' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task1:\n",
    "                qkv_scale_store_task1[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_1[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "        elif 'clip_vit_2' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task2:\n",
    "                qkv_scale_store_task2[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_2[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        # print(layer_idx, qkv)\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # print('hi')\n",
    "            scale_factors_1[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight.scale'] = concat_scale\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            scale_factors_2[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight' +\n",
    "                            '.scale'] = concat_scale\n",
    "\n",
    "    return scale_factors_1, scale_factors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shared_weight(shared_weight_state_dict, open_clip_state_dict_template):\n",
    "    qkv_store = {}\n",
    "    for old_key, value in shared_weight_state_dict.items():\n",
    "        if 'diff' in old_key or 'scale_dict' in old_key:\n",
    "            continue\n",
    "\n",
    "        new_key = transform_key(old_key)\n",
    "        layer_idx = new_key.split('.')[4]\n",
    "\n",
    "        if layer_idx not in qkv_store:\n",
    "            qkv_store[layer_idx] = {'q': None, 'k': None, 'v': None}\n",
    "\n",
    "        weight_type = new_key.split('.')[-1]\n",
    "        # in_proj.weight (q, k, v)\n",
    "        if weight_type in ['q_weight', 'k_weight', 'v_weight']:\n",
    "            if args.scale_shared_weight:\n",
    "                scale_key = f'shared.attn.layer.{layer_idx}.{weight_type[0]}'\n",
    "                if scale_key in shared_weight_state_dict['scale_dict']:\n",
    "                    weight_scale_factor = shared_weight_state_dict['scale_dict'][scale_key]\n",
    "                    scaled_value = value / weight_scale_factor\n",
    "                    qkv_store[layer_idx][weight_type[0]] = scaled_value\n",
    "                else:\n",
    "                    print(f\"Scale key {scale_key} not found in scale_dict.\")\n",
    "            else:\n",
    "                qkv_store[layer_idx][weight_type[0]] = value\n",
    "        else:  # out_proj.weight, c_fc.weight, c_proj.weight\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            weight_scale_factor = shared_weight_state_dict['scale_dict'][old_key]\n",
    "            open_clip_state_dict_template[new_key] = value / \\\n",
    "                weight_scale_factor\n",
    "\n",
    "    for layer_idx, qkv in qkv_store.items():\n",
    "        if all(v.bool().all().item() for v in qkv.values()):\n",
    "            in_proj_weight = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # concat qkv into 3072*1024 tensor\n",
    "            new_key = f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight'\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = in_proj_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Missing q, k, or v for layer {layer_idx}. q: {qkv['q']}, k: {qkv['k']}, v: {qkv['v']}\")\n",
    "\n",
    "    return open_clip_state_dict_template\n",
    "\n",
    "#나머지 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (model): CLIP(\n",
       "    (visual): VisualTransformer(\n",
       "      (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): Transformer(\n",
       "        (resblocks): ModuleList(\n",
       "          (0-23): 24 x ResidualAttentionBlock(\n",
       "            (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (ln_attn): Identity()\n",
       "            (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Sequential(\n",
       "              (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (ln): Identity()\n",
       "              (gelu): QuickGELU()\n",
       "              (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (token_embedding): Embedding(49408, 768)\n",
       "    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_weight_state_dict = torch.load('/data2/david3684/2024_arithmetic/checkpoints/rankmin_config_20241017_uni_vanilla_0.bin')\n",
    "\n",
    "scale_factors_1, scale_factors_2 = save_scale_factors(\n",
    "    shared_weight_state_dict['scale_dict'])\n",
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "\n",
    "zero_shot_encoder = ImageEncoder(args, keep_lang=False)\n",
    "\n",
    "#이러면 pretrained checkpoint에 있는 ln, bias 등으로 초기화 될것이다.\n",
    "formatted_shared_weight = format_shared_weight(shared_weight_state_dict, zero_shot_encoder.state_dict())\n",
    "\n",
    "zero_shot_encoder.load_state_dict(formatted_shared_weight)\n",
    "zero_shot_encoder.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 2048\n"
     ]
    }
   ],
   "source": [
    "_, _, val_preprocess = open_clip.create_model_and_transforms(\n",
    "            args.model, pretrained='openai', cache_dir=args.openclip_cachedir)\n",
    "dataset_1 = get_dataset(\n",
    "        args.tasks[0],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=None,\n",
    "    )\n",
    "dataloader_1 = get_dataloader(\n",
    "    dataset_1, is_train=False, args=args, image_encoder=None)\n",
    "\n",
    "dataset_2 = get_dataset(\n",
    "        args.tasks[1],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=args.num_test_samples,\n",
    "    )\n",
    "dataloader_2 = get_dataloader(\n",
    "    dataset_2, is_train=False, args=args, image_encoder=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:17<04:19, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:18<01:51,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:20<01:04,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:21<00:42,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:22<00:30,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [00:24<00:23,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [00:25<00:18,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:27<00:14,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:28<00:11,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:30<00:09,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:31<00:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [00:32<00:06,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [00:34<00:04,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [00:35<00:02,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [00:37<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:39<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SUN397. Accuracy: 82.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': 0.82275390625},\n",
       " tensor([[118],\n",
       "         [295],\n",
       "         [226],\n",
       "         ...,\n",
       "         [ 16],\n",
       "         [393],\n",
       "         [300]], device='cuda:0'),\n",
       " tensor([118, 295, 226,  ...,  16, 393, 300], device='cuda:0'))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = False\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, zero_shot_encoder.state_dict(), model_2.state_dict(), 'SUN397').to(args.device)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with no shared weight\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:37<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SUN397. Accuracy: 84.18%\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:41<00:00,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SUN397. Accuracy: 84.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'top1': 0.841796875},\n",
       " tensor([[118],\n",
       "         [295],\n",
       "         [226],\n",
       "         ...,\n",
       "         [ 16],\n",
       "         [393],\n",
       "         [300]], device='cuda:0'),\n",
       " tensor([118, 295, 226,  ...,  16, 393, 300], device='cuda:0'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.task_scale_factors = None\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = True\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, averaged_model.state_dict(), model_2.state_dict(), 'SUN397')\n",
    "eval_single_dataset_with_prediction(model_2, 'SUN397', dataloader_2, args)\n",
    "# eval_single_dataset(model_1, 'DTD', args)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\n",
    "# single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with no shared weight\n",
      "Building task vector with no shared weight\n"
     ]
    }
   ],
   "source": [
    "# starting from average weight\n",
    "task_vectors = {}\n",
    "args.initial_rank_ratio = 0.6\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weight = True\n",
    "    task_vectors[task] = TaskVector(args, averaged_model.state_dict(), finetuned_state_dict, task)\n",
    "\n",
    "task_vector_sum = sum(task_vectors.values())\n",
    "multi_task_encoder = task_vector_sum.apply_to(deepcopy(averaged_model), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     low_rank_task_vectors[task] \u001b[38;5;241m=\u001b[39m TaskVector(args, zero_shot_encoder\u001b[38;5;241m.\u001b[39mstate_dict(), finetuned_state_dict, task)\n\u001b[1;32m      6\u001b[0m low_rank_task_vector_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(low_rank_task_vectors\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m----> 7\u001b[0m low_rank_multi_task_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mlow_rank_task_vector_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero_shot_encoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/david3684/2024_arithmetic/src/task_vectors.py:152\u001b[0m, in \u001b[0;36mTaskVector.apply_to\u001b[0;34m(self, pretrained_checkpoint, scaling_coef)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m state_dict:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector:\n\u001b[0;32m--> 152\u001b[0m         state_dict[key] \u001b[38;5;241m=\u001b[39m \u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscaling_coef\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in task vector. Copying from pretrained model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "low_rank_task_vectors = {}\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weight = False\n",
    "    low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task)\n",
    "low_rank_task_vector_sum = sum(low_rank_task_vectors.values()).to(args.device)\n",
    "low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    if task == 'DTD':\n",
    "        args.pretrained_model = 'laion2b_s32b_b82k'\n",
    "    else:\n",
    "        args.pretrained_model = 'openai'\n",
    "    args.task_scale_factors = None\n",
    "    eval_single_dataset(multi_task_encoder, task, args)\n",
    "    args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "    print(args.task_scale_factors)\n",
    "    print(type(low_rank_multi_task_encoder))\n",
    "    eval_single_dataset(low_rank_multi_task_encoder, task, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
