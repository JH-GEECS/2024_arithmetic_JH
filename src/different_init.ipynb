{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/david3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "module_path = os.path.abspath(os.path.join('/data2/david3684/2024_arithmetic'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from src.eval import eval_single_dataset_with_prediction, eval_single_dataset\n",
    "from src.main import save_scale_factors\n",
    "from src.args import parse_arguments\n",
    "from src.datasets.common import get_dataloader, maybe_dictionarize\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.modeling import ImageEncoder, ImageClassifier\n",
    "from src.task_vectors import TaskVector\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = 'ViT-L-14'\n",
    "        self.tasks = ['DTD', 'SUN397']\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.task_scale_factors = None\n",
    "        self.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "        self.data_location = '/data2/david3684/data'\n",
    "        self.no_shared_weights = True\n",
    "        self.eval_datasets = None\n",
    "        self.train_dataset = None\n",
    "        self.exp_name = None\n",
    "        self.results_db = None\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.001\n",
    "        self.wd = 0.1\n",
    "        self.ls = 0.0\n",
    "        self.warmup_length = 500\n",
    "        self.epochs = 10\n",
    "        self.load = None\n",
    "        self.cache_dir = None\n",
    "        self.openclip_cachedir = '/data2/david3684/.cache/open_clip'\n",
    "        self.initial_rank_ratio = 1.0\n",
    "        self.low_rank_mode = 'SoRA'\n",
    "        self.pretrained_model = 'openai'\n",
    "        self.scale_shared_weight = True\n",
    "        self.no_shared_weight = True\n",
    "        self.num_test_samples = 2048\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/DTDVal/finetuned_laion2b_s32b_b82k.pt')\n",
    "model_2 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/SUN397/finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(state_dict1, state_dict2):\n",
    "    \"\"\"Average the weights of two state dicts.\"\"\"\n",
    "    averaged_state_dict = {}\n",
    "    for key in state_dict1:\n",
    "        averaged_state_dict[key] = (state_dict1[key] + state_dict2[key]) / 2\n",
    "    return averaged_state_dict\n",
    "\n",
    "def create_model_with_averaged_weights(args, state_dict1, state_dict2):\n",
    "    \"\"\"Create a model with averaged weights.\"\"\"\n",
    "    averaged_state_dict = average_weights(state_dict1, state_dict2)\n",
    "    model = ImageEncoder(args, keep_lang=False)\n",
    "    model.load_state_dict(averaged_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "averaged_model = create_model_with_averaged_weights(args, model_1.state_dict(), model_2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(old_key):\n",
    "    if old_key.startswith('shared.attn.layer') or old_key.startswith('clip_vit'):\n",
    "        parts = old_key.split('.')\n",
    "        layer_idx = parts[3]\n",
    "        # print(layer_idx)\n",
    "        sub_key = parts[4]\n",
    "        if sub_key in ['q', 'k', 'v']:\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.{sub_key}_weight'\n",
    "        elif sub_key == 'out_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.out_proj.weight'\n",
    "        elif sub_key == 'c_fc' or sub_key == 'c_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.mlp.{sub_key}.weight'\n",
    "    return old_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scale_factors(scale_dict):\n",
    "    qkv_scale_store_task1 = {}\n",
    "    qkv_scale_store_task2 = {}\n",
    "    scale_factors_1 = {}\n",
    "    scale_factors_2 = {}\n",
    "    for scale_dict_key, value in scale_dict.items():\n",
    "        transformed_scale_dict_key = transform_key(scale_dict_key)\n",
    "        if 'clip_vit_1' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task1:\n",
    "                qkv_scale_store_task1[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_1[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "        elif 'clip_vit_2' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task2:\n",
    "                qkv_scale_store_task2[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_2[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        # print(layer_idx, qkv)\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # print('hi')\n",
    "            scale_factors_1[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight.scale'] = concat_scale\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            scale_factors_2[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight' +\n",
    "                            '.scale'] = concat_scale\n",
    "\n",
    "    return scale_factors_1, scale_factors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shared_weight(shared_weight_state_dict, open_clip_state_dict_template):\n",
    "    qkv_store = {}\n",
    "    for old_key, value in shared_weight_state_dict.items():\n",
    "        if 'diff' in old_key or 'scale_dict' in old_key:\n",
    "            continue\n",
    "\n",
    "        new_key = transform_key(old_key)\n",
    "        layer_idx = new_key.split('.')[4]\n",
    "\n",
    "        if layer_idx not in qkv_store:\n",
    "            qkv_store[layer_idx] = {'q': None, 'k': None, 'v': None}\n",
    "\n",
    "        weight_type = new_key.split('.')[-1]\n",
    "        # in_proj.weight (q, k, v)\n",
    "        if weight_type in ['q_weight', 'k_weight', 'v_weight']:\n",
    "            if args.scale_shared_weight:\n",
    "                scale_key = f'shared.attn.layer.{layer_idx}.{weight_type[0]}'\n",
    "                if scale_key in shared_weight_state_dict['scale_dict']:\n",
    "                    weight_scale_factor = shared_weight_state_dict['scale_dict'][scale_key]\n",
    "                    scaled_value = value / weight_scale_factor\n",
    "                    qkv_store[layer_idx][weight_type[0]] = scaled_value\n",
    "                else:\n",
    "                    print(f\"Scale key {scale_key} not found in scale_dict.\")\n",
    "            else:\n",
    "                qkv_store[layer_idx][weight_type[0]] = value\n",
    "        else:  # out_proj.weight, c_fc.weight, c_proj.weight\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            weight_scale_factor = shared_weight_state_dict['scale_dict'][old_key]\n",
    "            open_clip_state_dict_template[new_key] = value / \\\n",
    "                weight_scale_factor\n",
    "\n",
    "    for layer_idx, qkv in qkv_store.items():\n",
    "        if all(v.bool().all().item() for v in qkv.values()):\n",
    "            in_proj_weight = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # concat qkv into 3072*1024 tensor\n",
    "            new_key = f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight'\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = in_proj_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Missing q, k, or v for layer {layer_idx}. q: {qkv['q']}, k: {qkv['k']}, v: {qkv['v']}\")\n",
    "\n",
    "    return open_clip_state_dict_template\n",
    "\n",
    "#나머지 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_weight_state_dict = torch.load('/data2/david3684/2024_arithmetic/checkpoints/rankmin_config_20241017_uni_vanilla_0.bin')\n",
    "\n",
    "scale_factors_1, scale_factors_2 = save_scale_factors(\n",
    "    shared_weight_state_dict['scale_dict'])\n",
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "\n",
    "zero_shot_encoder = ImageEncoder(args, keep_lang=False)\n",
    "\n",
    "#이러면 pretrained checkpoint에 있는 ln, bias 등으로 초기화 될것이다.\n",
    "formatted_shared_weight = format_shared_weight(shared_weight_state_dict, zero_shot_encoder.state_dict())\n",
    "\n",
    "zero_shot_encoder.load_state_dict(formatted_shared_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 2048\n"
     ]
    }
   ],
   "source": [
    "_, _, val_preprocess = open_clip.create_model_and_transforms(\n",
    "            args.model, pretrained='openai', cache_dir=args.openclip_cachedir)\n",
    "dataset_1 = get_dataset(\n",
    "        args.tasks[0],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=None,\n",
    "    )\n",
    "dataloader_1 = get_dataloader(\n",
    "    dataset_1, is_train=False, args=args, image_encoder=None)\n",
    "\n",
    "dataset_2 = get_dataset(\n",
    "        args.tasks[1],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=args.num_test_samples,\n",
    "    )\n",
    "dataloader_2 = get_dataloader(\n",
    "    dataset_2, is_train=False, args=args, image_encoder=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Making task vector for model.positional_embedding\n",
      "Making task vector for model.text_projection\n",
      "Making task vector for model.logit_scale\n",
      "Making task vector for model.visual.class_embedding\n",
      "Making task vector for model.visual.positional_embedding\n",
      "Making task vector for model.visual.proj\n",
      "Making task vector for model.visual.conv1.weight\n",
      "Making task vector for model.visual.ln_pre.weight\n",
      "Making task vector for model.visual.ln_pre.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_proj.bias\n",
      "Making task vector for model.visual.ln_post.weight\n",
      "Making task vector for model.visual.ln_post.bias\n",
      "Making task vector for model.token_embedding.weight\n",
      "Making task vector for model.ln_final.weight\n",
      "Making task vector for model.ln_final.bias\n",
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/tmp/ipykernel_947829/1413999176.py\u001b[0m(7)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      6 \u001b[0;31m\u001b[0mtask_vector_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaskVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_shot_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SUN397'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 7 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0meval_single_dataset_with_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SUN397'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "<src.task_vectors.TaskVector object at 0x7fb352912a10>\n",
      "dict_keys(['model.positional_embedding', 'model.text_projection', 'model.logit_scale', 'model.visual.class_embedding', 'model.visual.positional_embedding', 'model.visual.proj', 'model.visual.conv1.weight', 'model.visual.ln_pre.weight', 'model.visual.ln_pre.bias', 'model.visual.transformer.resblocks.0.ln_1.weight', 'model.visual.transformer.resblocks.0.ln_1.bias', 'model.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.visual.transformer.resblocks.0.ln_2.weight', 'model.visual.transformer.resblocks.0.ln_2.bias', 'model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.visual.transformer.resblocks.1.ln_1.weight', 'model.visual.transformer.resblocks.1.ln_1.bias', 'model.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.visual.transformer.resblocks.1.ln_2.weight', 'model.visual.transformer.resblocks.1.ln_2.bias', 'model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.visual.transformer.resblocks.2.ln_1.weight', 'model.visual.transformer.resblocks.2.ln_1.bias', 'model.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.visual.transformer.resblocks.2.ln_2.weight', 'model.visual.transformer.resblocks.2.ln_2.bias', 'model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.visual.transformer.resblocks.3.ln_1.weight', 'model.visual.transformer.resblocks.3.ln_1.bias', 'model.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.visual.transformer.resblocks.3.ln_2.weight', 'model.visual.transformer.resblocks.3.ln_2.bias', 'model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.visual.transformer.resblocks.4.ln_1.weight', 'model.visual.transformer.resblocks.4.ln_1.bias', 'model.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.visual.transformer.resblocks.4.ln_2.weight', 'model.visual.transformer.resblocks.4.ln_2.bias', 'model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.visual.transformer.resblocks.5.ln_1.weight', 'model.visual.transformer.resblocks.5.ln_1.bias', 'model.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.visual.transformer.resblocks.5.ln_2.weight', 'model.visual.transformer.resblocks.5.ln_2.bias', 'model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.visual.transformer.resblocks.6.ln_1.weight', 'model.visual.transformer.resblocks.6.ln_1.bias', 'model.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.visual.transformer.resblocks.6.ln_2.weight', 'model.visual.transformer.resblocks.6.ln_2.bias', 'model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.visual.transformer.resblocks.7.ln_1.weight', 'model.visual.transformer.resblocks.7.ln_1.bias', 'model.visual.transformer.resblocks.7.attn.in_proj_weight', 'model.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.visual.transformer.resblocks.7.ln_2.weight', 'model.visual.transformer.resblocks.7.ln_2.bias', 'model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.visual.transformer.resblocks.8.ln_1.weight', 'model.visual.transformer.resblocks.8.ln_1.bias', 'model.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.visual.transformer.resblocks.8.ln_2.weight', 'model.visual.transformer.resblocks.8.ln_2.bias', 'model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.visual.transformer.resblocks.9.ln_1.weight', 'model.visual.transformer.resblocks.9.ln_1.bias', 'model.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.visual.transformer.resblocks.9.attn.out_proj.weight', 'model.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.visual.transformer.resblocks.9.ln_2.weight', 'model.visual.transformer.resblocks.9.ln_2.bias', 'model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.visual.transformer.resblocks.10.ln_1.weight', 'model.visual.transformer.resblocks.10.ln_1.bias', 'model.visual.transformer.resblocks.10.attn.in_proj_weight', 'model.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.visual.transformer.resblocks.10.ln_2.weight', 'model.visual.transformer.resblocks.10.ln_2.bias', 'model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.12.ln_1.weight', 'model.visual.transformer.resblocks.12.ln_1.bias', 'model.visual.transformer.resblocks.12.attn.in_proj_weight', 'model.visual.transformer.resblocks.12.attn.in_proj_bias', 'model.visual.transformer.resblocks.12.attn.out_proj.weight', 'model.visual.transformer.resblocks.12.attn.out_proj.bias', 'model.visual.transformer.resblocks.12.ln_2.weight', 'model.visual.transformer.resblocks.12.ln_2.bias', 'model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'model.visual.transformer.resblocks.13.ln_1.weight', 'model.visual.transformer.resblocks.13.ln_1.bias', 'model.visual.transformer.resblocks.13.attn.in_proj_weight', 'model.visual.transformer.resblocks.13.attn.in_proj_bias', 'model.visual.transformer.resblocks.13.attn.out_proj.weight', 'model.visual.transformer.resblocks.13.attn.out_proj.bias', 'model.visual.transformer.resblocks.13.ln_2.weight', 'model.visual.transformer.resblocks.13.ln_2.bias', 'model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'model.visual.transformer.resblocks.14.ln_1.weight', 'model.visual.transformer.resblocks.14.ln_1.bias', 'model.visual.transformer.resblocks.14.attn.in_proj_weight', 'model.visual.transformer.resblocks.14.attn.in_proj_bias', 'model.visual.transformer.resblocks.14.attn.out_proj.weight', 'model.visual.transformer.resblocks.14.attn.out_proj.bias', 'model.visual.transformer.resblocks.14.ln_2.weight', 'model.visual.transformer.resblocks.14.ln_2.bias', 'model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'model.visual.transformer.resblocks.15.ln_1.weight', 'model.visual.transformer.resblocks.15.ln_1.bias', 'model.visual.transformer.resblocks.15.attn.in_proj_weight', 'model.visual.transformer.resblocks.15.attn.in_proj_bias', 'model.visual.transformer.resblocks.15.attn.out_proj.weight', 'model.visual.transformer.resblocks.15.attn.out_proj.bias', 'model.visual.transformer.resblocks.15.ln_2.weight', 'model.visual.transformer.resblocks.15.ln_2.bias', 'model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'model.visual.transformer.resblocks.16.ln_1.weight', 'model.visual.transformer.resblocks.16.ln_1.bias', 'model.visual.transformer.resblocks.16.attn.in_proj_weight', 'model.visual.transformer.resblocks.16.attn.in_proj_bias', 'model.visual.transformer.resblocks.16.attn.out_proj.weight', 'model.visual.transformer.resblocks.16.attn.out_proj.bias', 'model.visual.transformer.resblocks.16.ln_2.weight', 'model.visual.transformer.resblocks.16.ln_2.bias', 'model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'model.visual.transformer.resblocks.17.ln_1.weight', 'model.visual.transformer.resblocks.17.ln_1.bias', 'model.visual.transformer.resblocks.17.attn.in_proj_weight', 'model.visual.transformer.resblocks.17.attn.in_proj_bias', 'model.visual.transformer.resblocks.17.attn.out_proj.weight', 'model.visual.transformer.resblocks.17.attn.out_proj.bias', 'model.visual.transformer.resblocks.17.ln_2.weight', 'model.visual.transformer.resblocks.17.ln_2.bias', 'model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'model.visual.transformer.resblocks.18.ln_1.weight', 'model.visual.transformer.resblocks.18.ln_1.bias', 'model.visual.transformer.resblocks.18.attn.in_proj_weight', 'model.visual.transformer.resblocks.18.attn.in_proj_bias', 'model.visual.transformer.resblocks.18.attn.out_proj.weight', 'model.visual.transformer.resblocks.18.attn.out_proj.bias', 'model.visual.transformer.resblocks.18.ln_2.weight', 'model.visual.transformer.resblocks.18.ln_2.bias', 'model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'model.visual.transformer.resblocks.19.ln_1.weight', 'model.visual.transformer.resblocks.19.ln_1.bias', 'model.visual.transformer.resblocks.19.attn.in_proj_weight', 'model.visual.transformer.resblocks.19.attn.in_proj_bias', 'model.visual.transformer.resblocks.19.attn.out_proj.weight', 'model.visual.transformer.resblocks.19.attn.out_proj.bias', 'model.visual.transformer.resblocks.19.ln_2.weight', 'model.visual.transformer.resblocks.19.ln_2.bias', 'model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'model.visual.transformer.resblocks.20.ln_1.weight', 'model.visual.transformer.resblocks.20.ln_1.bias', 'model.visual.transformer.resblocks.20.attn.in_proj_weight', 'model.visual.transformer.resblocks.20.attn.in_proj_bias', 'model.visual.transformer.resblocks.20.attn.out_proj.weight', 'model.visual.transformer.resblocks.20.attn.out_proj.bias', 'model.visual.transformer.resblocks.20.ln_2.weight', 'model.visual.transformer.resblocks.20.ln_2.bias', 'model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'model.visual.transformer.resblocks.21.ln_1.weight', 'model.visual.transformer.resblocks.21.ln_1.bias', 'model.visual.transformer.resblocks.21.attn.in_proj_weight', 'model.visual.transformer.resblocks.21.attn.in_proj_bias', 'model.visual.transformer.resblocks.21.attn.out_proj.weight', 'model.visual.transformer.resblocks.21.attn.out_proj.bias', 'model.visual.transformer.resblocks.21.ln_2.weight', 'model.visual.transformer.resblocks.21.ln_2.bias', 'model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'model.visual.transformer.resblocks.22.ln_1.weight', 'model.visual.transformer.resblocks.22.ln_1.bias', 'model.visual.transformer.resblocks.22.attn.in_proj_weight', 'model.visual.transformer.resblocks.22.attn.in_proj_bias', 'model.visual.transformer.resblocks.22.attn.out_proj.weight', 'model.visual.transformer.resblocks.22.attn.out_proj.bias', 'model.visual.transformer.resblocks.22.ln_2.weight', 'model.visual.transformer.resblocks.22.ln_2.bias', 'model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'model.visual.transformer.resblocks.23.ln_1.weight', 'model.visual.transformer.resblocks.23.ln_1.bias', 'model.visual.transformer.resblocks.23.attn.in_proj_weight', 'model.visual.transformer.resblocks.23.attn.in_proj_bias', 'model.visual.transformer.resblocks.23.attn.out_proj.weight', 'model.visual.transformer.resblocks.23.attn.out_proj.bias', 'model.visual.transformer.resblocks.23.ln_2.weight', 'model.visual.transformer.resblocks.23.ln_2.bias', 'model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'model.visual.ln_post.weight', 'model.visual.ln_post.bias', 'model.token_embedding.weight', 'model.ln_final.weight', 'model.ln_final.bias'])\n",
      "tensor([[-2.2229e-05,  6.7377e-06, -2.5527e-04,  ..., -1.7403e-03,\n",
      "         -1.1927e-03, -1.6457e-07],\n",
      "        [ 1.0178e-05, -1.2688e-05,  7.8335e-04,  ...,  4.5888e-04,\n",
      "          1.4063e-03, -3.6578e-06],\n",
      "        [ 2.2185e-05,  8.3631e-06, -1.2331e-03,  ..., -9.9640e-04,\n",
      "          1.3787e-03, -8.8191e-06],\n",
      "        ...,\n",
      "        [ 4.5559e-05,  8.3942e-07, -2.6713e-03,  ...,  8.1216e-04,\n",
      "         -2.2524e-04, -8.1700e-06],\n",
      "        [ 6.3537e-06,  4.2729e-05, -1.3890e-03,  ..., -1.0654e-03,\n",
      "          2.2099e-04,  7.7882e-06],\n",
      "        [ 5.0836e-06, -3.0157e-05,  2.4891e-03,  ..., -1.0468e-03,\n",
      "         -5.2123e-06,  1.1544e-05]])\n",
      "*** invalid literal for int() with base 10: \"= task_vector_temp.vector['model.visual.transformer.resblocks.0.attn.in_proj_weight']\"\n",
      "*** invalid literal for int() with base 10: '.norm()'\n",
      "*** invalid literal for int() with base 10: '.norm'\n",
      "*** NameError: name 'w' is not defined\n",
      "*** invalid literal for int() with base 10: \"= task_vector_temp.vector['model.visual.transformer.resblocks.0.attn.in_proj_weight']\"\n",
      "tensor([[-2.2229e-05,  6.7377e-06, -2.5527e-04,  ..., -1.7403e-03,\n",
      "         -1.1927e-03, -1.6457e-07],\n",
      "        [ 1.0178e-05, -1.2688e-05,  7.8335e-04,  ...,  4.5888e-04,\n",
      "          1.4063e-03, -3.6578e-06],\n",
      "        [ 2.2185e-05,  8.3631e-06, -1.2331e-03,  ..., -9.9640e-04,\n",
      "          1.3787e-03, -8.8191e-06],\n",
      "        ...,\n",
      "        [ 4.5559e-05,  8.3942e-07, -2.6713e-03,  ...,  8.1216e-04,\n",
      "         -2.2524e-04, -8.1700e-06],\n",
      "        [ 6.3537e-06,  4.2729e-05, -1.3890e-03,  ..., -1.0654e-03,\n",
      "          2.2099e-04,  7.7882e-06],\n",
      "        [ 5.0836e-06, -3.0157e-05,  2.4891e-03,  ..., -1.0468e-03,\n",
      "         -5.2123e-06,  1.1544e-05]])\n",
      "tensor(1.4115)\n"
     ]
    }
   ],
   "source": [
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = False\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, zero_shot_encoder.state_dict(), model_2.state_dict(), 'SUN397')\n",
    "import ipdb; ipdb.set_trace()\n",
    "eval_single_dataset_with_prediction(model_2, 'SUN397', dataloader_2, args)\n",
    "# eval_single_dataset(model_1, 'DTD', args)\n",
    "# single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mtask_scale_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args\u001b[38;5;241m.\u001b[39mpretrained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mno_shared_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args.task_scale_factors = None\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = True\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, averaged_model.state_dict(), model_2.state_dict(), 'SUN397')\n",
    "import ipdb; ipdb.set_trace()\n",
    "eval_single_dataset_with_prediction(model_2, 'SUN397', dataloader_2, args)\n",
    "# eval_single_dataset(model_1, 'DTD', args)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\n",
    "# single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from average weight\n",
    "task_vectors = {}\n",
    "args.initial_rank_ratio = 0.6\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weights = True\n",
    "    task_vectors[task] = TaskVector(args, averaged_model.state_dict(), finetuned_state_dict, task)\n",
    "\n",
    "task_vector_sum = sum(task_vectors.values())\n",
    "multi_task_encoder = task_vector_sum.apply_to(deepcopy(averaged_model), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rank_task_vectors = {}\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weights = False\n",
    "    low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task)\n",
    "low_rank_task_vector_sum = sum(low_rank_task_vectors.values())\n",
    "low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    if task == 'DTD':\n",
    "        args.pretrained_model = 'laion2b_s32b_b82k'\n",
    "    else:\n",
    "        args.pretrained_model = 'openai'\n",
    "    args.task_scale_factors = None\n",
    "    eval_single_dataset(multi_task_encoder, task, args)\n",
    "    args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "    print(args.task_scale_factors)\n",
    "    print(type(low_rank_multi_task_encoder))\n",
    "    eval_single_dataset(low_rank_multi_task_encoder, task, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
