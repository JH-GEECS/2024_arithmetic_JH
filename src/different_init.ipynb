{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/david3684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "module_path = os.path.abspath(os.path.join('/data2/david3684/2024_arithmetic'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from src.eval import eval_single_dataset_with_prediction, eval_single_dataset\n",
    "from src.main import save_scale_factors\n",
    "from src.args import parse_arguments\n",
    "from src.datasets.common import get_dataloader, maybe_dictionarize\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.modeling import ImageEncoder, ImageClassifier\n",
    "from src.task_vectors import TaskVector\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = 'ViT-L-14'\n",
    "        self.tasks = ['DTD', 'SUN397']\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.task_scale_factors = None\n",
    "        self.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "        self.data_location = '/data2/david3684/data'\n",
    "        self.no_shared_weights = True\n",
    "        self.eval_datasets = None\n",
    "        self.train_dataset = None\n",
    "        self.exp_name = None\n",
    "        self.results_db = None\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.001\n",
    "        self.wd = 0.1\n",
    "        self.ls = 0.0\n",
    "        self.warmup_length = 500\n",
    "        self.epochs = 10\n",
    "        self.load = None\n",
    "        self.cache_dir = None\n",
    "        self.openclip_cachedir = '/data2/david3684/.cache/open_clip'\n",
    "        self.initial_rank_ratio = 1.0\n",
    "        self.low_rank_mode = 'SoRA'\n",
    "        self.pretrained_model = 'openai'\n",
    "        self.scale_shared_weight = True\n",
    "        self.no_shared_weight = True\n",
    "        self.num_test_samples = 2048\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/DTDVal/finetuned_laion2b_s32b_b82k.pt')\n",
    "model_2 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/SUN397/finetuned.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(state_dict1, state_dict2):\n",
    "    \"\"\"Average the weights of two state dicts.\"\"\"\n",
    "    averaged_state_dict = {}\n",
    "    for key in state_dict1:\n",
    "        averaged_state_dict[key] = (state_dict1[key] + state_dict2[key]) / 2\n",
    "    return averaged_state_dict\n",
    "\n",
    "def create_model_with_averaged_weights(args, state_dict1, state_dict2):\n",
    "    \"\"\"Create a model with averaged weights.\"\"\"\n",
    "    averaged_state_dict = average_weights(state_dict1, state_dict2)\n",
    "    model = ImageEncoder(args, keep_lang=False)\n",
    "    model.load_state_dict(averaged_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    }
   ],
   "source": [
    "averaged_model = create_model_with_averaged_weights(args, model_1.state_dict(), model_2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(old_key):\n",
    "    if old_key.startswith('shared.attn.layer') or old_key.startswith('clip_vit'):\n",
    "        parts = old_key.split('.')\n",
    "        layer_idx = parts[3]\n",
    "        # print(layer_idx)\n",
    "        sub_key = parts[4]\n",
    "        if sub_key in ['q', 'k', 'v']:\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.{sub_key}_weight'\n",
    "        elif sub_key == 'out_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.out_proj.weight'\n",
    "        elif sub_key == 'c_fc' or sub_key == 'c_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.mlp.{sub_key}.weight'\n",
    "    return old_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scale_factors(scale_dict):\n",
    "    qkv_scale_store_task1 = {}\n",
    "    qkv_scale_store_task2 = {}\n",
    "    scale_factors_1 = {}\n",
    "    scale_factors_2 = {}\n",
    "    for scale_dict_key, value in scale_dict.items():\n",
    "        transformed_scale_dict_key = transform_key(scale_dict_key)\n",
    "        if 'clip_vit_1' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task1:\n",
    "                qkv_scale_store_task1[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task1[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_1[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "        elif 'clip_vit_2' in scale_dict_key:\n",
    "            subkey = scale_dict_key.split('.')[-1]\n",
    "            index = scale_dict_key.split('.')[-2]\n",
    "            if index not in qkv_scale_store_task2:\n",
    "                qkv_scale_store_task2[index] = {\n",
    "                    'q': None, 'k': None, 'v': None}\n",
    "            if subkey == 'q':\n",
    "                q_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['q'] = q_scale\n",
    "            elif subkey == 'k':\n",
    "                k_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['k'] = k_scale\n",
    "            elif subkey == 'v':\n",
    "                v_scale = value.unsqueeze(0)\n",
    "                qkv_scale_store_task2[index]['v'] = v_scale\n",
    "            else:\n",
    "                scale_factors_2[transformed_scale_dict_key +\n",
    "                                '.scale'] = value  # scale factor 저장\n",
    "\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        # print(layer_idx, qkv)\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # print('hi')\n",
    "            scale_factors_1[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight.scale'] = concat_scale\n",
    "    for layer_idx, qkv in qkv_scale_store_task1.items():\n",
    "        if qkv['q'] is not None and qkv['k'] is not None and qkv['v'] is not None:\n",
    "            concat_scale = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            scale_factors_2[f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight' +\n",
    "                            '.scale'] = concat_scale\n",
    "\n",
    "    return scale_factors_1, scale_factors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shared_weight(shared_weight_state_dict, open_clip_state_dict_template):\n",
    "    qkv_store = {}\n",
    "    for old_key, value in shared_weight_state_dict.items():\n",
    "        if 'diff' in old_key or 'scale_dict' in old_key:\n",
    "            continue\n",
    "\n",
    "        new_key = transform_key(old_key)\n",
    "        layer_idx = new_key.split('.')[4]\n",
    "\n",
    "        if layer_idx not in qkv_store:\n",
    "            qkv_store[layer_idx] = {'q': None, 'k': None, 'v': None}\n",
    "\n",
    "        weight_type = new_key.split('.')[-1]\n",
    "        # in_proj.weight (q, k, v)\n",
    "        if weight_type in ['q_weight', 'k_weight', 'v_weight']:\n",
    "            if args.scale_shared_weight:\n",
    "                scale_key = f'shared.attn.layer.{layer_idx}.{weight_type[0]}'\n",
    "                if scale_key in shared_weight_state_dict['scale_dict']:\n",
    "                    weight_scale_factor = shared_weight_state_dict['scale_dict'][scale_key]\n",
    "                    scaled_value = value / weight_scale_factor\n",
    "                    qkv_store[layer_idx][weight_type[0]] = scaled_value\n",
    "                else:\n",
    "                    print(f\"Scale key {scale_key} not found in scale_dict.\")\n",
    "            else:\n",
    "                qkv_store[layer_idx][weight_type[0]] = value\n",
    "        else:  # out_proj.weight, c_fc.weight, c_proj.weight\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            weight_scale_factor = shared_weight_state_dict['scale_dict'][old_key]\n",
    "            open_clip_state_dict_template[new_key] = value / \\\n",
    "                weight_scale_factor\n",
    "\n",
    "    for layer_idx, qkv in qkv_store.items():\n",
    "        if all(v.bool().all().item() for v in qkv.values()):\n",
    "            in_proj_weight = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # concat qkv into 3072*1024 tensor\n",
    "            new_key = f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight'\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = in_proj_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Missing q, k, or v for layer {layer_idx}. q: {qkv['q']}, k: {qkv['k']}, v: {qkv['v']}\")\n",
    "\n",
    "    return open_clip_state_dict_template\n",
    "\n",
    "#나머지 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_weight_state_dict = torch.load('/data2/david3684/2024_arithmetic/checkpoints/rankmin_config_20241017_uni_vanilla_0.bin')\n",
    "\n",
    "scale_factors_1, scale_factors_2 = save_scale_factors(\n",
    "    shared_weight_state_dict['scale_dict'])\n",
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "\n",
    "zero_shot_encoder = ImageEncoder(args, keep_lang=False)\n",
    "\n",
    "#이러면 pretrained checkpoint에 있는 ln, bias 등으로 초기화 될것이다.\n",
    "formatted_shared_weight = format_shared_weight(shared_weight_state_dict, zero_shot_encoder.state_dict())\n",
    "\n",
    "zero_shot_encoder.load_state_dict(formatted_shared_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 2048\n"
     ]
    }
   ],
   "source": [
    "_, _, val_preprocess = open_clip.create_model_and_transforms(\n",
    "            args.model, pretrained='openai', cache_dir=args.openclip_cachedir)\n",
    "dataset_1 = get_dataset(\n",
    "        args.tasks[0],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=None,\n",
    "    )\n",
    "dataloader_1 = get_dataloader(\n",
    "    dataset_1, is_train=False, args=args, image_encoder=None)\n",
    "\n",
    "dataset_2 = get_dataset(\n",
    "        args.tasks[1],\n",
    "        val_preprocess,\n",
    "        location=args.data_location,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=16,\n",
    "        num_test_samples=args.num_test_samples,\n",
    "    )\n",
    "dataloader_2 = get_dataloader(\n",
    "    dataset_2, is_train=False, args=args, image_encoder=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Making task vector for model.positional_embedding\n",
      "Making task vector for model.text_projection\n",
      "Making task vector for model.logit_scale\n",
      "Making task vector for model.visual.class_embedding\n",
      "Making task vector for model.visual.positional_embedding\n",
      "Making task vector for model.visual.proj\n",
      "Making task vector for model.visual.conv1.weight\n",
      "Making task vector for model.visual.ln_pre.weight\n",
      "Making task vector for model.visual.ln_pre.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.12.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.13.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.14.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.15.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.16.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.17.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.18.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.19.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.20.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.21.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.22.mlp.c_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_1.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_1.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.in_proj_weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.in_proj_bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.out_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.attn.out_proj.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_2.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.ln_2.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_fc.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_fc.bias\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_proj.weight\n",
      "Making task vector for model.visual.transformer.resblocks.23.mlp.c_proj.bias\n",
      "Making task vector for model.visual.ln_post.weight\n",
      "Making task vector for model.visual.ln_post.bias\n",
      "Making task vector for model.token_embedding.weight\n",
      "Making task vector for model.ln_final.weight\n",
      "Making task vector for model.ln_final.bias\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                            | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████████▎                                                                                                                                                                        | 1/16 [00:17<04:25, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████████████████▌                                                                                                                                                             | 2/16 [00:19<01:53,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█████████████████████████████████▊                                                                                                                                                  | 3/16 [00:20<01:05,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████████████████████████████                                                                                                                                       | 4/16 [00:21<00:43,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████████████████████████████████████████████████▎                                                                                                                           | 5/16 [00:23<00:30,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████████████████████████████████████████████████████████▌                                                                                                                | 6/16 [00:24<00:23,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|██████████████████████████████████████████████████████████████████████████████▊                                                                                                     | 7/16 [00:26<00:18,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████████                                                                                          | 8/16 [00:27<00:14,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                              | 9/16 [00:28<00:11,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                   | 10/16 [00:30<00:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                        | 11/16 [00:31<00:07,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 12/16 [00:33<00:06,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 13/16 [00:34<00:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 14/16 [00:35<00:02,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 15/16 [00:37<00:01,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass with scaling factors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:39<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SUN397. Accuracy: 0.20%\n",
      "Classification head for ViT-L-14 on SUN397 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SUN397_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                            | 0/16 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x7f8a25111f30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                            | 0/16 [00:18<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# eval_single_dataset(model_1, 'DTD', args)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m single_task_encoder \u001b[38;5;241m=\u001b[39m task_vector_temp\u001b[38;5;241m.\u001b[39mapply_to(deepcopy(zero_shot_encoder), scaling_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43meval_single_dataset_with_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_task_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSUN397\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/david3684/2024_arithmetic/src/eval.py:28\u001b[0m, in \u001b[0;36meval_single_dataset_with_prediction\u001b[0;34m(image_encoder, dataset_name, dataloader, args, head)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     27\u001b[0m     top1, correct, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(dataloader)):\n\u001b[1;32m     29\u001b[0m         data \u001b[38;5;241m=\u001b[39m maybe_dictionarize(data)\n\u001b[1;32m     30\u001b[0m         x \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = False\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, zero_shot_encoder.state_dict(), model_2.state_dict(), 'SUN397')\n",
    "import ipdb; ipdb.set_trace()\n",
    "eval_single_dataset_with_prediction(model_2, 'SUN397', dataloader_2, args)\n",
    "# eval_single_dataset(model_1, 'DTD', args)\n",
    "# single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mtask_scale_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args\u001b[38;5;241m.\u001b[39mpretrained_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m args\u001b[38;5;241m.\u001b[39mno_shared_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "args.task_scale_factors = None\n",
    "args.pretrained_model = 'openai'\n",
    "args.no_shared_weight = True\n",
    "args.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "task_vector_temp = TaskVector(args, averaged_model.state_dict(), model_2.state_dict(), 'SUN397')\n",
    "import ipdb; ipdb.set_trace()\n",
    "eval_single_dataset_with_prediction(model_2, 'SUN397', dataloader_2, args)\n",
    "# eval_single_dataset(model_1, 'DTD', args)\n",
    "single_task_encoder = task_vector_temp.apply_to(deepcopy(averaged_model), scaling_coef=1.0)\n",
    "# single_task_encoder = task_vector_temp.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "eval_single_dataset_with_prediction(single_task_encoder, 'SUN397', dataloader_2, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from average weight\n",
    "task_vectors = {}\n",
    "args.initial_rank_ratio = 0.6\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weights = True\n",
    "    task_vectors[task] = TaskVector(args, averaged_model.state_dict(), finetuned_state_dict, task)\n",
    "\n",
    "task_vector_sum = sum(task_vectors.values())\n",
    "multi_task_encoder = task_vector_sum.apply_to(deepcopy(averaged_model), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rank_task_vectors = {}\n",
    "for task in args.tasks:\n",
    "    finetuned_state_dict = model_1.state_dict() if task == 'DTD' else model_2.state_dict()\n",
    "    args.no_shared_weights = False\n",
    "    low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task)\n",
    "low_rank_task_vector_sum = sum(low_rank_task_vectors.values())\n",
    "low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in args.tasks:\n",
    "    if task == 'DTD':\n",
    "        args.pretrained_model = 'laion2b_s32b_b82k'\n",
    "    else:\n",
    "        args.pretrained_model = 'openai'\n",
    "    args.task_scale_factors = None\n",
    "    eval_single_dataset(multi_task_encoder, task, args)\n",
    "    args.task_scale_factors = {\n",
    "    'DTD': scale_factors_1, 'SUN397': scale_factors_2}\n",
    "    print(args.task_scale_factors)\n",
    "    print(type(low_rank_multi_task_encoder))\n",
    "    eval_single_dataset(low_rank_multi_task_encoder, task, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
