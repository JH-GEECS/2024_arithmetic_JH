{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data2/david3684/2024_arithmetic/src\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "print(os.getcwd())\n",
    "module_path = os.path.abspath(os.path.join('/data2/david3684/2024_arithmetic'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from src.eval import eval_single_dataset_with_prediction, eval_single_dataset\n",
    "from src.main import save_scale_factors\n",
    "from src.args import parse_arguments\n",
    "from src.datasets.common import get_dataloader, maybe_dictionarize\n",
    "from src.datasets.registry import get_dataset\n",
    "from src.modeling import ImageEncoder, ImageClassifier\n",
    "from src.task_vectors import TaskVector\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import open_clip\n",
    "\n",
    "import csv\n",
    "import ray\n",
    "from easydict import EasyDict\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cars, DTD, EuroSAT, GTSRB, MNIST, RESISC45, SUN397, SVHN\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model = 'ViT-L-14'\n",
    "        self.tasks = [\"Cars\", \"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"SUN397\", \"SVHN\"] \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.task_scale_factors = None\n",
    "        self.save = '/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14'\n",
    "        self.data_location = '/data1/common_datasets/vision_cls/'\n",
    "        self.eval_datasets = None\n",
    "        self.train_dataset = None\n",
    "        self.exp_name = None\n",
    "        self.results_db = None\n",
    "        self.batch_size = 256\n",
    "        self.lr = 0.001\n",
    "        self.wd = 0.1\n",
    "        self.ls = 0.0\n",
    "        self.warmup_length = 500\n",
    "        self.epochs = 10\n",
    "        self.load = None\n",
    "        self.cache_dir = None\n",
    "        self.openclip_cachedir = '/data2/david3684/.cache/open_clip'\n",
    "        self.initial_rank_ratio = 1.0\n",
    "        self.low_rank_mode = 'SoRA'\n",
    "        self.pretrained_model = 'openai'\n",
    "        self.scale_shared_weight = False\n",
    "        self.num_test_samples = 2048\n",
    "        self.no_shared_weight = False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned weight\n",
    "[\"Cars\", \"DTD\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"SUN397\", \"SVHN\"]\n",
    "\n",
    "model_list = {}\n",
    "for idx, name in enumerate(args.tasks):\n",
    "    model = torch.load(f'/data1/common_datasets/shared_weight/task_vector/ViT-L-14/{name}/finetuned.pt').to(args.device)\n",
    "    model_list[name] = model\n",
    "\n",
    "# model_1 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/DTD/finetuned.pt').to(args.device)\n",
    "# model_2 = torch.load('/data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/SUN397/finetuned.pt').to(args.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(old_key):\n",
    "    if old_key.startswith('shared.attn.layer') or old_key.startswith('clip_vit'):\n",
    "        parts = old_key.split('.')\n",
    "        layer_idx = parts[3]\n",
    "        # print(layer_idx)\n",
    "        sub_key = parts[4]\n",
    "        if sub_key in ['q', 'k', 'v']:\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.{sub_key}_weight'\n",
    "        elif sub_key == 'out_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.attn.out_proj.weight'\n",
    "        elif sub_key == 'c_fc' or sub_key == 'c_proj':\n",
    "            return f'model.visual.transformer.resblocks.{layer_idx}.mlp.{sub_key}.weight'\n",
    "    return old_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shared_weight(shared_weight_state_dict, open_clip_state_dict_template):\n",
    "    qkv_store = {}\n",
    "    for old_key, value in shared_weight_state_dict.items():\n",
    "        if 'diff' in old_key or 'scale_dict' in old_key:\n",
    "            continue\n",
    "\n",
    "        new_key = transform_key(old_key)\n",
    "        layer_idx = new_key.split('.')[4]\n",
    "\n",
    "        if layer_idx not in qkv_store:\n",
    "            qkv_store[layer_idx] = {'q': None, 'k': None, 'v': None}\n",
    "\n",
    "        weight_type = new_key.split('.')[-1]\n",
    "        # in_proj.weight (q, k, v)\n",
    "        if weight_type in ['q_weight', 'k_weight', 'v_weight']:\n",
    "            qkv_store[layer_idx][weight_type[0]] = value\n",
    "        else:  # out_proj.weight, c_fc.weight, c_proj.weight\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = value\n",
    "\n",
    "    for layer_idx, qkv in qkv_store.items():\n",
    "        if all(v.bool().all().item() for v in qkv.values()):\n",
    "            in_proj_weight = torch.cat([qkv['q'], qkv['k'], qkv['v']], dim=0)\n",
    "            # concat qkv into 3072*1024 tensor\n",
    "            new_key = f'model.visual.transformer.resblocks.{layer_idx}.attn.in_proj_weight'\n",
    "            assert new_key in open_clip_state_dict_template\n",
    "            open_clip_state_dict_template[new_key] = in_proj_weight\n",
    "        else:\n",
    "            print(\n",
    "                f\"Missing q, k, or v for layer {layer_idx}. q: {qkv['q']}, k: {qkv['k']}, v: {qkv['v']}\")\n",
    "\n",
    "    return open_clip_state_dict_template\n",
    "\n",
    "#나머지 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L-14 pre-trained weights.\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# format shaed weight into openclip state dict\n",
    "shared_weight_state_dict = torch.load('/data1/common_datasets/shared_weight/20241025/vanilla_T8/rankmin_config_20241025_uni_T8_vanilla.bin')\n",
    "zero_shot_encoder = ImageEncoder(args, keep_lang=False)\n",
    "\n",
    "formatted_shared_weight = format_shared_weight(shared_weight_state_dict, zero_shot_encoder.state_dict())\n",
    "print(zero_shot_encoder.load_state_dict(formatted_shared_weight))\n",
    "zero_shot_encoder = zero_shot_encoder.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n"
     ]
    }
   ],
   "source": [
    "low_rank_task_vectors = {}\n",
    "for task in args.tasks:\n",
    "    model = model_list[task]\n",
    "    finetuned_state_dict = model.state_dict()\n",
    "    low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task).to(args.device)\n",
    "\n",
    "low_rank_task_vector_sum = sum(low_rank_task_vectors.values())  \n",
    "low_rank_single_task_encoder = low_rank_task_vectors[task].apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "\n",
    "eval_single_dataset(low_rank_single_task_encoder, \"Cars\", deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:40:24,362\tINFO worker.py:1786 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Single task model for Cars fetched\n",
      "Multi task model for Cars fetched\n",
      "Single task model for Cars fetched\n",
      "Multi task model for Cars fetched\n",
      "Single task model for Cars fetched\n",
      "Multi task model for Cars fetched\n",
      "Building task vector with shared weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:21,797\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523786, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523786, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:23,797\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523786, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523786, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:24,797\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523768, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523768, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:28,808\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523767, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523767, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single task model for Cars fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:30,811\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523783, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523783, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi task model for Cars fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:34,924\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single task model for Cars fetched\n",
      "Multi task model for Cars fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:37,819\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single task model for Cars fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:39,812\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi task model for Cars fetched\n",
      "Building task vector with shared weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 20:41:41,805\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:43,806\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:45,807\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n",
      "2024-10-24 20:41:47,807\tERROR worker.py:409 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "RuntimeError: The remote function failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment or cannot be found from sys.path ['/data2/david3684/2024_arithmetic/src', '/home/david3684/.local/lib/python3.10/site-packages/ray/thirdparty_files', '/home/david3684/.local/lib/python3.10/site-packages/ray/_private/workers', '/data2/david3684/2024_arithmetic/src', '/data2/david3684/Diffusion-Assignment3-ControlNet-LoRA/task_2_lora', '/opt/conda/lib/python310.zip', '/opt/conda/lib/python3.10', '/opt/conda/lib/python3.10/lib-dynload', '/home/david3684/.local/lib/python3.10/site-packages', '/opt/conda/lib/python3.10/site-packages']:\n",
      "\n",
      "\u001b[36mray::eval_single_task_wrapper()\u001b[39m (pid=2523771, ip=172.17.0.9)\n",
      "ModuleNotFoundError: No module named 'src'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m low_rank_single_task_encoder \u001b[38;5;241m=\u001b[39m low_rank_task_vectors[task]\u001b[38;5;241m.\u001b[39mapply_to(deepcopy(zero_shot_encoder), scaling_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# single_task_reuslt = eval_single_dataset(low_rank_single_task_encoder, task, deepcopy(args))\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m ray_pack\u001b[38;5;241m.\u001b[39mappend(\u001b[43meval_ray_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_rank_single_task_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meach_scale_factor\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSingle task model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fetched\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Evaluate multi task model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# print('Multi task model')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/remote_function.py:250\u001b[0m, in \u001b[0;36mRemoteFunction.options.<locals>.FuncWrapper.remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdated_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/util/tracing/tracing_helper.py:310\u001b[0m, in \u001b[0;36m_tracing_task_invocation.<locals>._invocation_remote_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    313\u001b[0m tracer \u001b[38;5;241m=\u001b[39m _opentelemetry\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mget_tracer(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/remote_function.py:468\u001b[0m, in \u001b[0;36mRemoteFunction._remote\u001b[0;34m(self, args, kwargs, **task_options)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator(invocation)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/remote_function.py:435\u001b[0m, in \u001b[0;36mRemoteFunction._remote.<locals>.invocation\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m worker\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mLOCAL_MODE:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    433\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_cross_language\n\u001b[1;32m    434\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross language remote function cannot be executed locally.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 435\u001b[0m object_refs \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_descriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlist_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exception_allowlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebugger_breakpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized_runtime_env_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_backpressure_num_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_task_events\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Reset worker's debug context from the last \"remote\" command\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# (which applies only to this .remote call).\u001b[39;00m\n\u001b[1;32m    453\u001b[0m worker\u001b[38;5;241m.\u001b[39mdebugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:4028\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.submit_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:4032\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.submit_task\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:827\u001b[0m, in \u001b[0;36mray._raylet.prepare_args_and_increment_put_refs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:877\u001b[0m, in \u001b[0;36mray._raylet.prepare_args_internal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/serialization.py:519\u001b[0m, in \u001b[0;36mSerializationContext.serialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RawSerializedObject(value)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_msgpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/serialization.py:497\u001b[0m, in \u001b[0;36mSerializationContext._serialize_to_msgpack\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m python_objects:\n\u001b[1;32m    496\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m ray_constants\u001b[38;5;241m.\u001b[39mOBJECT_METADATA_TYPE_PYTHON\n\u001b[0;32m--> 497\u001b[0m     pickle5_serialized_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serialize_to_pickle5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_objects\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     pickle5_serialized_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/serialization.py:439\u001b[0m, in \u001b[0;36mSerializationContext._serialize_to_pickle5\u001b[0;34m(self, metadata, value)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_in_band_serialization()\n\u001b[0;32m--> 439\u001b[0m     inband \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_callback\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_and_clear_contained_object_refs()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py:1479\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m   1478\u001b[0m     cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[0;32m-> 1479\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py:1245\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/storage.py:937\u001b[0m, in \u001b[0;36mTypedStorage.__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__reduce__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    936\u001b[0m     b \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m--> 937\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_use_new_zipfile_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_load_from_bytes, (b\u001b[38;5;241m.\u001b[39mgetvalue(),))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:633\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m--> 633\u001b[0m         \u001b[43m_legacy_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:777\u001b[0m, in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m serialized_storage_keys:\n\u001b[1;32m    776\u001b[0m     storage, dtype \u001b[38;5;241m=\u001b[39m serialized_storages[key]\n\u001b[0;32m--> 777\u001b[0m     \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_should_read_directly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ray runner\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def eval_single_task_wrapper(is_single, encoder_state_dict, task, args, scale_coef):\n",
    "    \n",
    "    encoder = ImageEncoder(args, keep_lang=False)\n",
    "    encoder.load_state_dict(encoder_state_dict)\n",
    "    result = eval_single_dataset(encoder, task, args)\n",
    "\n",
    "    if is_single:\n",
    "        print(f\"Running single task evaluation for {task}: {result}\")\n",
    "    else:\n",
    "        print(f\"Running multitask task evaluation for {task}: {result}\")\n",
    "\n",
    "    ret = {\n",
    "        \"is_single\": is_single,\n",
    "        \"task\": task,\n",
    "        \"top1\": result[\"top1\"],\n",
    "        \"scaling_coef\": 1.0,\n",
    "        \"initial_rank_ratio_list\": args.initial_rank_ratio,\n",
    "    }\n",
    "    if not is_single:\n",
    "        ret[\"scaling_coef\"] = scale_coef\n",
    "\n",
    "    return ret\n",
    "\n",
    "eval_ray_runner = ray.remote(eval_single_task_wrapper).options(num_gpus=0.5, num_cpus=4)\n",
    "ray_pack = []\n",
    "\n",
    "experiment_vector = EasyDict()\n",
    "experiment_vector.initial_rank_ratio_list = [0.0, 0.001, 0.005, 0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.5, 0.64, 1.0]\n",
    "experiment_vector.scaling_coef_list = [0.2, 0.5, 1.0]\n",
    "\n",
    "for initial_rank_ratio in experiment_vector.initial_rank_ratio_list:\n",
    "    print(f'Initial rank ratio: {initial_rank_ratio}')\n",
    "    args.initial_rank_ratio = initial_rank_ratio\n",
    "    low_rank_task_vectors = {}\n",
    "    # Build low rank task vectors\n",
    "    for task in args.tasks:\n",
    "        model = model_list[task]\n",
    "        finetuned_state_dict = model.state_dict()\n",
    "        low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task).to(args.device)\n",
    "    \n",
    "    low_rank_task_vector_sum = sum(low_rank_task_vectors.values())  \n",
    "    \n",
    "    for task in args.tasks:\n",
    "        for each_scale_factor in experiment_vector.scaling_coef_list: \n",
    "            # print(f'Evaluate {task}')\n",
    "            # Evaluate sinlge task model\n",
    "            low_rank_single_task_encoder = low_rank_task_vectors[task].apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "            # single_task_reuslt = eval_single_dataset(low_rank_single_task_encoder, task, deepcopy(args))\n",
    "            ray_pack.append(eval_ray_runner.remote(True, low_rank_single_task_encoder.state_dict(), task, deepcopy(args), each_scale_factor))\n",
    "            print(f'Single task model for {task} fetched')\n",
    "            \n",
    "            # Evaluate multi task model\n",
    "            # print('Multi task model')\n",
    "            low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=each_scale_factor)\n",
    "            # multi_task_result = eval_single_dataset(low_rank_multi_task_encoder, task, deepcopy(args))\n",
    "            ray_pack.append(eval_ray_runner.remote(False, low_rank_multi_task_encoder.state_dict(), task, deepcopy(args), each_scale_factor))\n",
    "            print(f'Multi task model for {task} fetched')\n",
    "\n",
    "results = ray.get(ray_pack)\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "csv_file_path = f\"./eval_results_{current_time}.csv\"\n",
    "with open(csv_file_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "    fieldnames = [\"is_single\", \"task\", \"top1\", \"initial_rank_ratio_list\", \"scaling_coef\"]\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        writer.writerow(result)\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n",
      "Building task vector with shared weight\n"
     ]
    }
   ],
   "source": [
    "# loop for task vector rank\n",
    "\n",
    "experiment_vector = EasyDict()\n",
    "experiment_vector.initial_rank_ratio_list = [1.0]\n",
    "experiment_vector.scaling_coef_list = [1.0, 0.3]\n",
    "\n",
    "for initial_rank_ratio in experiment_vector.initial_rank_ratio_list:\n",
    "    args.initial_rank_ratio = initial_rank_ratio\n",
    "    low_rank_task_vectors = {}\n",
    "    \n",
    "    # Build low rank task vectors\n",
    "    for task in args.tasks:\n",
    "        model = model_list[task]\n",
    "        finetuned_state_dict = model.state_dict()\n",
    "        low_rank_task_vectors[task] = TaskVector(args, zero_shot_encoder.state_dict(), finetuned_state_dict, task).to(args.device)\n",
    "    \n",
    "    low_rank_task_vector_sum = sum(low_rank_task_vectors.values())  \n",
    "\n",
    "    # 여기 ray로 parallelize 시켜버리고 값 wandb logging하게 만든 다음에\n",
    "    # ray.get()으로 받아서 csv로 쓰게 만들어 버려야 겠다.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find classification head for ViT-L-14 on EuroSAT at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_EuroSAT_openai.pt, building one from scratch.\n",
      "Loading ViT-L-14 pre-trained weights.\n",
      "Building classification head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving classification head to /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_EuroSAT_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:43<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on EuroSAT. Accuracy: 99.93%\n",
      "Classification head for ViT-L-14 on GTSRB exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_GTSRB_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_GTSRB_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:34<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on GTSRB. Accuracy: 99.24%\n",
      "Classification head for ViT-L-14 on MNIST exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_MNIST_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_MNIST_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [02:03<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on MNIST. Accuracy: 99.69%\n",
      "Classification head for ViT-L-14 on RESISC45 exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_RESISC45_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_RESISC45_openai.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on RESISC45. Accuracy: 2.70%\n",
      "Classification head for ViT-L-14 on SVHN exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SVHN_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_SVHN_openai.pt\n",
      "Using downloaded and verified file: /data1/common_datasets/vision_cls/svhn/train_32x32.mat\n",
      "Using downloaded and verified file: /data1/common_datasets/vision_cls/svhn/test_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [05:01<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating on SVHN. Accuracy: 98.12%\n",
      "Classification head for ViT-L-14 on DTD exists at /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Loading classification head from /data2/david3684/2024_arithmetic/checkpoints/ViT-L-14/head_DTD_openai.pt\n",
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:28<01:26, 14.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEuroSAT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTSRB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESISC45\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVHN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDTD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUN397\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCars\u001b[39m\u001b[38;5;124m\"\u001b[39m,]:\n\u001b[1;32m      2\u001b[0m     low_rank_single_task_encoder \u001b[38;5;241m=\u001b[39m low_rank_task_vectors[task]\u001b[38;5;241m.\u001b[39mapply_to(deepcopy(zero_shot_encoder), scaling_coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     single_task_reuslt \u001b[38;5;241m=\u001b[39m \u001b[43meval_single_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow_rank_single_task_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/david3684/2024_arithmetic/src/eval.py:82\u001b[0m, in \u001b[0;36meval_single_dataset\u001b[0;34m(image_encoder, dataset_name, args)\u001b[0m\n\u001b[1;32m     78\u001b[0m     logits \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_logits(x, model, dataset_name, args)\n\u001b[1;32m     80\u001b[0m     pred \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 82\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     86\u001b[0m top1 \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m n\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for task in [\"DTD\", \"SUN397\", \"Cars\", \"EuroSAT\", \"GTSRB\", \"MNIST\", \"RESISC45\", \"SVHN\", ]:\n",
    "    low_rank_single_task_encoder = low_rank_task_vectors[task].apply_to(deepcopy(zero_shot_encoder), scaling_coef=1.0)\n",
    "    single_task_reuslt = eval_single_dataset(low_rank_single_task_encoder, task, deepcopy(args))\n",
    "        \n",
    "    # for each_scale_factor in experiment_vector.scaling_coef_list: \n",
    "    #     print(f'Evaluate {task}')\n",
    "    #     # Evaluate sinlge task model\n",
    "    #     print('Single task model')\n",
    "\n",
    "    #     # Evaluate multi task model\n",
    "    #     print('Multi task model')\n",
    "    #     low_rank_multi_task_encoder = low_rank_task_vector_sum.apply_to(deepcopy(zero_shot_encoder), scaling_coef=each_scale_factor)\n",
    "    #     multi_task_result = eval_single_dataset(low_rank_multi_task_encoder, task, deepcopy(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n",
      "Train dataset size: 1880\n",
      "Test dataset size: 1880\n",
      "Downloading and loading the SUN397 dataset...\n",
      "Number of classes: 397\n",
      "Train dataset size: 87003\n",
      "Test dataset size: 2048\n"
     ]
    }
   ],
   "source": [
    "# _, _, val_preprocess = open_clip.create_model_and_transforms(\n",
    "#             args.model, pretrained='openai', cache_dir=args.openclip_cachedir)\n",
    "# dataset_1 = get_dataset(\n",
    "#         args.tasks[0],\n",
    "#         val_preprocess,\n",
    "#         location=args.data_location,\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=16,\n",
    "#         num_test_samples=None,\n",
    "#     )\n",
    "# dataloader_1 = get_dataloader(\n",
    "#     dataset_1, is_train=False, args=args, image_encoder=None)\n",
    "\n",
    "# dataset_2 = get_dataset(\n",
    "#         args.tasks[1],\n",
    "#         val_preprocess,\n",
    "#         location=args.data_location,\n",
    "#         batch_size=args.batch_size,\n",
    "#         num_workers=16,\n",
    "#         num_test_samples=args.num_test_samples,\n",
    "#     )\n",
    "# dataloader_2 = get_dataloader(\n",
    "#     dataset_2, is_train=False, args=args, image_encoder=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
